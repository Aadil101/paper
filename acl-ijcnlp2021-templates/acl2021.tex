%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{titlesec}
\setcounter{secnumdepth}{5}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{BigGreen at LCP 2021: \\
Lexical Complexity Prediction with Feature Engineering and Learning}

\author{
  Aadil Islam\\
  Department of Computer Science\\
  Dartmouth College\\
  \texttt{aadil.islam.21@dartmouth.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Here, I will write the abstract. This will be even briefer than the small description that I've previously submitted.
\end{abstract}

\section{Introduction}

\begin{itemize}
  \item Why lexical complexity prediction is important in the real world, namely via applications to readability, text simplication, etc.
  \item What the previous 2016 and 2018 SemEval tasks aimed to accomplish, the assumptions of said tasks, compared to this year's task, namely transition from binary task and probabilistic component to use of 5-point Likert scale. Do not go into findings from 2016 and 2018 tasks, yet.
  \item What models we submitted, overview of the sections of this paper, and a link to GitHub repo of our code.
\end{itemize}

\section{Related Work}

\begin{itemize}
  \item What previous papers showed (Mc Laughlin 1969, Dale and Chall 1948, Shardlow 2013, Paetzold and Specia 2016, Yimam et al. 2018).
  \item What 2016 task showed, overall findings of the authors, and shortcomings. 
  \item What 2018 task showed, overall findings of the authors, and shortcomings.
  \item What specific approaches from the above inspired us, namely our feature set, our use of BERT, and our ensembling techniques. Basically, substantiating our choices.
\end{itemize}

\section{Datasets}

\subsection{CompLex Dataset}

\begin{itemize}
  \item What advantages of CompLex Dataset are over past datasets.
  \item Number of instances breakdown (table) of single token (train, trial, test) and multi token (train, trial, test) by subcorpus (bible, biomed, europarl).
  \item Complexity distribution breakdown (table) of single token (train, trial, test) and multi token (train, trial, test) by subcorpus (bible, biomed, europarl).
  \item Mention that whenever we perform cross-validation upon each train set, we always stratify by class and corpus.
  \item Show actual samples that inspired us to pick particular features. 
  \begin{itemize}
    \item Eg. showing easy vs. difficult target words with respect to term frequency.
    \item Eg. showing easy vs. difficult target words with respect to it being proper noun or not.
    \item Eg. showing easy vs. difficult target words with respect to context (show the complexities of a target word under different contexts)
    \item ...
  \end{itemize}
\end{itemize}

\subsection{External Datasets}

\begin{itemize}
  \item Briefly credit external corpora used to extract term frequency-based features:
  \item English Gigaword corpus, Google Books Ngram Dataset (version 2) via PhraseFinder API, British National Corpus, SUBTLEXus.
\end{itemize}

\section{BigGreen Systems}

\begin{itemize}
  \item Introduce the two models that we designed.
  \item Explain that our final predictions for single token subtask and multi word-expression subtasks are ensembles of these predictions of these models
  \item How we essentially treat the multi word-expression subtask as a special case of the single token subtask approach.
\end{itemize}

\subsection{Feature Engineering-Based System}

\subsubsection{Feature Engineering}

\begin{itemize}
  \item Mention that for the multi word-expression subtask, we compute the following features like for the single token subtask, but for the head and tail tokens independently.
  \item Mention that we also consider the log-transformed version of each feature described below, due numerous features in our feature set following power law distribution.
\end{itemize}

\paragraph{Lexical Features}

\begin{itemize}
  \item Word length, Number of syllables, is\_acronym, is\_pronoun
\end{itemize}
  
\paragraph{Semantic Features}

\begin{itemize}
  \item Number of hyponyms, Number of hypernyms, GloVe word embeddings, ELMo word embeddings, GloVe context embeddings, InferSent embeddings 
\end{itemize}

\paragraph{Phonetic Features}

\begin{itemize}
  \item Phoneme transition probability, Character transition probability
\end{itemize}

\paragraph{Word Frequency and N-gram}

\begin{itemize}
  \item Gigaword TF, Gigaword TF Lemma, Gigaword TF-IDF, Gigaword Summed BPE TFs, Gigaword Number of OOV words, Gigaword Summed TF bigrams, Gigaword Summed TF trigrams, Gigaword Summed TF-IDF bigrams, Gigaword Summed TF-IDF trigrams, Google TF sum of bigrams, Google TF std of trigrams, Google TF head bigram, Google TF tail bigram, Google TF min bigram, Google TF max bigram, Google TF sum of trigrams, Google TF std of trigrams, Google TF head trigram, Google TF tail trigram, Google TF min trigram, Google TF max trigram, BNC TF, SUBTLEXus FREQcount, SUBTLEXus CDlow, SUBTLwf, SUBTLcd
\end{itemize}

\paragraph{Syntactic Features}

\begin{itemize}
  \item POS tag of target word, Depth of constituency parse tree, Depth of target word in constituency parse tree, Number of words at depth of target word
\end{itemize}

\paragraph{Readability Metrics}

\begin{itemize}
  \item SMOGIndex, DaleChallIndex, etc.
\end{itemize}

\subsubsection{Feature Selection}

\begin{itemize}
  \item Introduce how we trained models using multiple combinations of the following feature selection strategies.
\end{itemize}

\paragraph{Filter Methods}

\begin{itemize}
  \item How we use Standard Scaler to normalize features, as per sklearn ML framework.
  \item How we remove features that have constant/quasi-constant variance.
  \item How we rank features based on mutual information with lexical complexity, and select the top-$N$ from features, where $N$ is corresponds to knee-of-curve.
  \item How we experimented with removing features based on VIF.
\end{itemize}

\paragraph{Wrapper Methods}

\begin{itemize}
  \item How we tried Forward Feature Selection (FFS) to select features greedily, as opposed to selecting all features.
\end{itemize}

\paragraph{Embedded Methods}

\begin{itemize}
  \item How we tried Lasso and ElasticNet regression with tuning of regularization parameters via grid search.
\end{itemize}

\subsubsection{Training}

\begin{itemize}
  \item How we tried the said feature selection strategies through experiments on the train set, and found ensemble-based techniques regressors such as XGBoost to be the more successful algorithms, though simple linear regression performs reasonably well too.
  \item How we fit two regressors, the first upon the full train set, and second on a train set with Very Easy, Easy, and Neutral samples reduced (aiming to better predict difficult and very difficult words due).
\end{itemize}

\subsection{Feature Learning and Transfer Learning-Based System}
\begin{itemize}
  \item Reiterate the potential influence of context on lexical complexity.
  \item What MT-DNN is and its advantages, namely the ability to initialize with BERT pretrained weights.
\end{itemize}

\subsubsection{Architecture}
\begin{itemize}
  \item How BERT works and its advantages.
  \item Specify that we are using BERT cased base model, cased being preferable to uncased according to experiments.
\end{itemize}

\subsubsection{Input Layer}
\begin{itemize}
  \item For the single token subtask, how we pass sentence-target word pairs into MT-DNN model.
  \item For the multi token subtask, how we analogously pass sentence-phrase pairs into the MT-DNN model.
  \item How inputs are tokenized by BERT.
\end{itemize}

\subsubsection{Output Layer}
\begin{itemize}
  \item What the MT-DNN model produces.
  \item Explain choice to use predictions of the MT-DNN model as opposed to, say, the averaged BERT embeddings of the final layers.
  \item What is BERT attention mechanism, and how one can interpret it.
\end{itemize}

\subsection{Ensembles}
\begin{itemize}
  \item For single token subtask, how we compute a weighted average of the both feature engineering-based model predictions (ie. predictions with and predictions without data reduction) and the feature learning-based model predictions.
  \item For multi token subtask, how we harness the feature engineering-based and feature learning-based models fitted for the single token subtask, to predict two sets of predicted complexities for the head and tail words.
  \item How we then compute a weighted average of the predicted complexities of the head and tail words, and the predicted complexities of the feature engineering-based model.
  \item Note that for both subtasks, we tune weights using cross validation upon each train set.
\end{itemize}

\section{Results}

\begin{itemize}
  \item Describe baseline systems.
  \item Show table of results (on trial and test sets) for all single token subtask models (for each of the feature selection techniques we tried) with MAE, Pearson, Spearman, and competition ranking.
  \item Show table of results (on trial and test sets) for all multi word-expression subtask models (for both disappointing and improved models) with MAE, Pearson, Spearman, and competition ranking.
  \item Include baseline system performances in each table.
\end{itemize}

\section{Analysis}

\subsection{Performance}

\begin{itemize}
  \item How each model performed in relation to another.
  \item How the disappointing multi word-expression model was designed, why it struggled, and what inspired our changes to it.
  \item Why latest model struggles on trial set but succeeds on cross validation on the train set (and on the test set).
\end{itemize}

\subsection{Feature Contribution}

\begin{itemize}
  \item Show table of feature importance scores for the best performing model single token model.
  \item Discuss the most important features.
\end{itemize}

\subsection{BERT Attention}

\begin{itemize}
  \item (If we have room) Show heatmap of attention head importances (attention weights averaged across all samples).
  \item For 2-3 of most important attention heads, show the attention maps for actual samples.
  \item What (if any) patterns do the most important attention heads demonstrate.
  \item Whether these patterns surprising/expected in the context of predicting lexical complexity.
\end{itemize}

\section{Conclusion}
\begin{itemize}
  \item Any avenues for improvement, including synthetic data generation to improve Pearson correlation on extremely difficult samples.
\end{itemize}

\section*{Acknowledgments}

\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021}

%\appendix

\end{document}
