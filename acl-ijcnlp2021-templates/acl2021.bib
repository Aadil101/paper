@inbook{Fellbaum:2005,
    author  = {Christiane Fellbaum},
    title   = {Encyclopedia of Language and Linguistics, Second Edition},
    year    = "2005",
    chapter = {WordNet and wordnets},
    publisher = {Oxford: Elsevier}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@inproceedings{Peters:2018,
  author={Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title={Deep contextualized word representations},
  booktitle={Proc. of NAACL},
  year={2018}
}

@InProceedings{conneau-EtAl:2017:EMNLP2017,
  author    = {Conneau, Alexis  and  Kiela, Douwe  and  Schwenk, Holger  and  Barrault, Lo\"{i}c  and  Bordes, Antoine},
  title     = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {670--680},
  url       = {https://www.aclweb.org/anthology/D17-1070}
}

@article{gooding_kochmar_2018, 
  title     = {CAMB at CWI Shared Task 2018: Complex Word Identification with Ensemble-Based Voting}, 
  DOI       = {10.18653/v1/w18-0520}, 
  journal   = {Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications}, 
  author    = {Gooding, Sian and Kochmar, Ekaterina}, 
  year      = {2018}
}

@misc{phrasefinder,
  author       = {Martin Trenkmann},
  title        = {{PhraseFinder} -- Search millions of books for language use},
  howpublished = {\url{https://phrasefinder.io}},
  note         = {Accessed: 2021-02-08}
}

@INPROCEEDINGS{Loper02nltk:the,
    author = {Edward Loper and Steven Bird},
    title = {NLTK: The Natural Language Toolkit},
    booktitle = {In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics},
    year = {2002}
}

@InProceedings{manning-EtAl:2014:P14-5,
  author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
  title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
  year      = {2014},
  pages     = {55--60},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@article{10.2307/1473669,
 ISSN = {15554023},
 URL = {http://www.jstor.org/stable/1473669},
 author = {Edgar Dale and Jeanne S. Chall},
 journal = {Educational Research Bulletin},
 number = {2},
 pages = {37--54},
 publisher = {Taylor & Francis, Ltd.},
 title = {A Formula for Predicting Readability: Instructions},
 volume = {27},
 year = {1948}
}

@book{gunning1952technique,
  title={The Technique of Clear Writing},
  author={Gunning, R.},
  lccn={51013126},
  url={https://books.google.com/books?id=ofI0AAAAMAAJ},
  year={1952},
  publisher={McGraw-Hill}
}

@article{Tibshirani.x,
author = {Tibshirani, Robert},
title = {Regression Shrinkage and Selection Via the Lasso},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {58},
number = {1},
pages = {267-288},
keywords = {quadratic programming, regression, shrinkage, subset selection},
doi = {https://doi.org/10.1111/j.2517-6161.1996.tb02080.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1996.tb02080.x},
abstract = {SUMMARY We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
year = {1996}
}

@article{10.2307/3647580,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 volume = {67},
 year = {2005}
}

